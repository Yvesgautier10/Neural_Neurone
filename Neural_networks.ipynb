{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b028589-4c29-48e1-989f-517e865871ef",
   "metadata": {},
   "source": [
    "# construction d'un reseau de neuronnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "359b71f6-9133-4759-ab8a-67bffc26e55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75e80a12-c95b-45a1-806a-ceec5187115a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction d'un MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "820f5466-41a0-433e-9e3c-3aa03a59e8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-18 14:06:27.883222: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    layers.Input(shape=(28, 28)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844aea91-6f21-45e8-97ea-77f94522eeea",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f05489a-93bd-4df2-ac3e-fc8b77b91947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP plus robuste\n",
    "model1 = Sequential([\n",
    "    layers.Input(shape=(28, 28)),\n",
    "\n",
    "    #bloc 1\n",
    "    layers.Dense(128, activation='relu', kernel_initializer='he_normal'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    \n",
    "    #bloc 2\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.2),\n",
    "\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af05b9-b821-4466-8fe5-c3cacc03df33",
   "metadata": {},
   "source": [
    "# Le CNN robuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f6c6583-3d83-4fd2-9cd7-17679c87f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential([\n",
    "    layers.Input(shape=(28,28,5)),\n",
    "    \n",
    "    layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1a6ac5-a27e-4a0c-9074-f1eb5fa5b07d",
   "metadata": {},
   "source": [
    "# Séries Temporelles & Texte (Le RNN / LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb823dab-bd01-4064-bb95-f19c3db3221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential([\n",
    "    layers.Input(shape=(20,20)),\n",
    "    layers.LSTM(64, return_sequences=True),\n",
    "    layers.Dropout(0.2),\n",
    "\n",
    "    layers.LSTM(64, return_sequences=False),\n",
    "\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "    \n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4ae9f4b-8199-45ba-b82e-21b514ed3f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "# --- DONNÉES DE TEST ---\n",
    "data = \"\"\"Le chat mange la souris\n",
    "Le chien dort sur le tapis\n",
    "Le chat court dans le jardin\n",
    "Le chien mange sa gamelle\"\"\"\n",
    "\n",
    "# 1. Tokenization (Transformation des mots en nombres)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "total_words = len(tokenizer.word_index) + 1  # +1 pour le padding (0)\n",
    "\n",
    "# 2. Création de séquences (ex: \"Le chat\" -> \"mange\")\n",
    "input_sequences = []\n",
    "for line in data.split('\\n'):\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "# 3. Padding (S'assurer que toutes les séquences ont la même longueur)\n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# 4. Séparation X (Entrée) et Y (Cible)\n",
    "X, y = input_sequences[:,:-1], input_sequences[:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a86ff652-51bf-40dd-b9c2-2235debae2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yves/anaconda3/envs/tensflow/lib/python3.12/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_9\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_9\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                  │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_10 (\u001b[38;5;33mLSTM\u001b[0m)                  │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_10 (\u001b[38;5;33mDropout\u001b[0m)            │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_21 (\u001b[38;5;33mDense\u001b[0m)                │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    # Input_dim = taille du vocabulaire, output_dim = taille du vecteur de sens (32)\n",
    "    layers.Embedding(total_words, 32, input_length=max_sequence_len-1),\n",
    "    \n",
    "    # Couche LSTM (64 unités). On ne met pas Flatten car return_sequences=False par défaut\n",
    "    layers.LSTM(64),\n",
    "    \n",
    "    # Régularisation\n",
    "    layers.Dropout(0.2),\n",
    "    \n",
    "    # Sortie : Un neurone par mot dans le dictionnaire\n",
    "    layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer='adam', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "737d69bc-0f34-4840-9576-7784c2596f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phrase: 'Le chat' -> Suivant: chien\n",
      "Phrase: 'Le chien' -> Suivant: dort\n"
     ]
    }
   ],
   "source": [
    "# Entraînement\n",
    "model.fit(X, y, epochs=100, verbose=0) # 100 époques pour cet exemple minuscule\n",
    "\n",
    "def predire_mot_suivant(texte_depart):\n",
    "    # Transformer le texte en nombres\n",
    "    sequence = tokenizer.texts_to_sequences([texte_depart])[0]\n",
    "    sequence = pad_sequences([sequence], maxlen=max_sequence_len-1, padding='pre')\n",
    "    \n",
    "    # Prédire les probabilités\n",
    "    prediction = model.predict(sequence, verbose=0)\n",
    "    id_mot_predit = np.argmax(prediction)\n",
    "    \n",
    "    # Retrouver le mot correspondant à l'ID\n",
    "    for mot, index in tokenizer.word_index.items():\n",
    "        if index == id_mot_predit:\n",
    "            return mot\n",
    "    return \"\"\n",
    "\n",
    "# --- TEST ---\n",
    "print(f\"Phrase: 'Le chat' -> Suivant: {predire_mot_suivant('mange')}\")\n",
    "print(f\"Phrase: 'Le chien' -> Suivant: {predire_mot_suivant('Le chien')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e07d19ff-c3be-4ff8-9260-75310624dee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texte généré : Le chien dort sur le tapis\n"
     ]
    }
   ],
   "source": [
    "def generer_paragraphe(seed_text, nb_mots, model, tokenizer, max_sequence_len):\n",
    "    resultat = seed_text\n",
    "    \n",
    "    for _ in range(nb_mots):\n",
    "        # 1. Préparation de la séquence actuelle\n",
    "        token_list = tokenizer.texts_to_sequences([resultat])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        \n",
    "        # 2. Prédiction des probabilités pour le mot suivant\n",
    "        probabilities = model.predict(token_list, verbose=0)\n",
    "        \n",
    "        # 3. Récupération de l'index du mot le plus probable\n",
    "        predicted_index = np.argmax(probabilities, axis=-1)[0]\n",
    "        \n",
    "        # 4. Traduction de l'index en mot\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted_index:\n",
    "                output_word = word\n",
    "                break\n",
    "        \n",
    "        # 5. Ajout du mot au texte généré et répétition\n",
    "        resultat += \" \" + output_word\n",
    "        \n",
    "    return resultat\n",
    "\n",
    "# --- TEST DE GÉNÉRATION ---\n",
    "# On demande au modèle de générer 5 mots après \"Le chat\"\n",
    "paragraphe = generer_paragraphe(\"Le chien\", 4, model, tokenizer, max_sequence_len)\n",
    "print(f\"Texte généré : {paragraphe}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
